{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "from os.path import exists\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.examples.polyphonic_data_loader as poly\n",
    "import pyro.poutine as poutine\n",
    "from pyro.distributions import TransformedDistribution\n",
    "from pyro.distributions.transforms import affine_autoregressive\n",
    "from pyro.infer import (\n",
    "    SVI,\n",
    "    JitTrace_ELBO,\n",
    "    Trace_ELBO,\n",
    "    TraceEnum_ELBO,\n",
    "    TraceTMC_ELBO,\n",
    "    config_enumerate,\n",
    ")\n",
    "from pyro.optim import ClippedAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emitter(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes the bernoulli observation likelihood `p(x_t | z_t)`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, z_dim, emission_dim):\n",
    "        super().__init__()\n",
    "        # initialize the three linear transformations used in the neural network\n",
    "        self.lin_z_to_hidden = nn.Linear(z_dim, emission_dim)\n",
    "        self.lin_hidden_to_hidden = nn.Linear(emission_dim, emission_dim)\n",
    "        self.lin_hidden_to_input = nn.Linear(emission_dim, input_dim)\n",
    "        # initialize the two non-linearities used in the neural network\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, z_t):\n",
    "        \"\"\"\n",
    "        Given the latent z at a particular time step t we return the vector of\n",
    "        probabilities `ps` that parameterizes the bernoulli distribution `p(x_t|z_t)`\n",
    "        \"\"\"\n",
    "        h1 = self.relu(self.lin_z_to_hidden(z_t))\n",
    "        h2 = self.relu(self.lin_hidden_to_hidden(h1))\n",
    "        ps = torch.sigmoid(self.lin_hidden_to_input(h2))\n",
    "        return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedTransition(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes the gaussian latent transition probability `p(z_t | z_{t-1})`\n",
    "    See section 5 in the reference for comparison.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim, transition_dim):\n",
    "        super().__init__()\n",
    "        # initialize the six linear transformations used in the neural network\n",
    "        self.lin_gate_z_to_hidden = nn.Linear(z_dim, transition_dim)\n",
    "        self.lin_gate_hidden_to_z = nn.Linear(transition_dim, z_dim)\n",
    "        self.lin_proposed_mean_z_to_hidden = nn.Linear(z_dim, transition_dim)\n",
    "        self.lin_proposed_mean_hidden_to_z = nn.Linear(transition_dim, z_dim)\n",
    "        self.lin_sig = nn.Linear(z_dim, z_dim)\n",
    "        self.lin_z_to_loc = nn.Linear(z_dim, z_dim)\n",
    "        # modify the default initialization of lin_z_to_loc\n",
    "        # so that it's starts out as the identity function\n",
    "        self.lin_z_to_loc.weight.data = torch.eye(z_dim)\n",
    "        self.lin_z_to_loc.bias.data = torch.zeros(z_dim)\n",
    "        # initialize the three non-linearities used in the neural network\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "    def forward(self, z_t_1):\n",
    "        \"\"\"\n",
    "        Given the latent `z_{t-1}` corresponding to the time step t-1\n",
    "        we return the mean and scale vectors that parameterize the\n",
    "        (diagonal) gaussian distribution `p(z_t | z_{t-1})`\n",
    "        \"\"\"\n",
    "        # compute the gating function\n",
    "        _gate = self.relu(self.lin_gate_z_to_hidden(z_t_1))\n",
    "        gate = torch.sigmoid(self.lin_gate_hidden_to_z(_gate))\n",
    "        # compute the 'proposed mean'\n",
    "        _proposed_mean = self.relu(self.lin_proposed_mean_z_to_hidden(z_t_1))\n",
    "        proposed_mean = self.lin_proposed_mean_hidden_to_z(_proposed_mean)\n",
    "        # assemble the actual mean used to sample z_t, which mixes a linear transformation\n",
    "        # of z_{t-1} with the proposed mean modulated by the gating function\n",
    "        loc = (1 - gate) * self.lin_z_to_loc(z_t_1) + gate * proposed_mean\n",
    "        # compute the scale used to sample z_t, using the proposed mean from\n",
    "        # above as input the softplus ensures that scale is positive\n",
    "        scale = self.softplus(self.lin_sig(self.relu(proposed_mean)))\n",
    "        # return loc, scale which can be fed into Normal\n",
    "        return loc, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combiner(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameterizes `q(z_t | z_{t-1}, x_{t:T})`, which is the basic building block\n",
    "    of the guide (i.e. the variational distribution). The dependence on `x_{t:T}` is\n",
    "    through the hidden state of the RNN (see the PyTorch module `rnn` below)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim, rnn_dim):\n",
    "        super().__init__()\n",
    "        # initialize the three linear transformations used in the neural network\n",
    "        self.lin_z_to_hidden = nn.Linear(z_dim, rnn_dim)\n",
    "        self.lin_hidden_to_loc = nn.Linear(rnn_dim, z_dim)\n",
    "        self.lin_hidden_to_scale = nn.Linear(rnn_dim, z_dim)\n",
    "        # initialize the two non-linearities used in the neural network\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, z_t_1, h_rnn):\n",
    "        \"\"\"\n",
    "        Given the latent z at at a particular time step t-1 as well as the hidden\n",
    "        state of the RNN `h(x_{t:T})` we return the mean and scale vectors that\n",
    "        parameterize the (diagonal) gaussian distribution `q(z_t | z_{t-1}, x_{t:T})`\n",
    "        \"\"\"\n",
    "        # combine the rnn hidden state with a transformed version of z_t_1\n",
    "        h_combined = 0.5 * (self.tanh(self.lin_z_to_hidden(z_t_1)) + h_rnn)\n",
    "        # use the combined hidden state to compute the mean used to sample z_t\n",
    "        loc = self.lin_hidden_to_loc(h_combined)\n",
    "        # use the combined hidden state to compute the scale used to sample z_t\n",
    "        scale = self.softplus(self.lin_hidden_to_scale(h_combined))\n",
    "        # return loc, scale which can be fed into Normal\n",
    "        return loc, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMM(nn.Module):\n",
    "    \"\"\"\n",
    "    This PyTorch Module encapsulates the model as well as the\n",
    "    variational distribution (the guide) for the Deep Markov Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=88,\n",
    "        z_dim=32,\n",
    "        emission_dim=32,\n",
    "        transition_dim=32,\n",
    "        rnn_dim=32,\n",
    "        num_layers=1,\n",
    "        rnn_dropout_rate=0.0,\n",
    "        use_cuda=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # instantiate PyTorch modules used in the model and guide below\n",
    "        self.emitter = Emitter(input_dim, z_dim, emission_dim)\n",
    "        self.trans = GatedTransition(z_dim, transition_dim)\n",
    "        self.combiner = Combiner(z_dim, rnn_dim)\n",
    "        # dropout just takes effect on inner layers of rnn\n",
    "        rnn_dropout_rate = 0.0 if num_layers == 1 else rnn_dropout_rate\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=rnn_dim,\n",
    "            # nonlinearity=\"relu\",\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            num_layers=num_layers,\n",
    "            dropout=rnn_dropout_rate,\n",
    "        )\n",
    "\n",
    "        # if we're using normalizing flows, instantiate those too\n",
    "        # define a (trainable) parameters z_0 and z_q_0 that help define the probability\n",
    "        # distributions p(z_1) and q(z_1)\n",
    "        # (since for t = 1 there are no previous latents to condition on)\n",
    "        self.z_0 = nn.Parameter(torch.zeros(z_dim))\n",
    "        self.z_q_0 = nn.Parameter(torch.zeros(z_dim))\n",
    "        # define a (trainable) parameter for the initial hidden state of the rnn\n",
    "        self.h_0 = nn.Parameter(torch.zeros(1, 1, rnn_dim))\n",
    "\n",
    "        self.use_cuda = use_cuda\n",
    "        # if on gpu cuda-ize all PyTorch (sub)modules\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    # the model p(x_{1:T} | z_{1:T}) p(z_{1:T})\n",
    "    def model(\n",
    "        self,\n",
    "        mini_batch,\n",
    "        mini_batch_reversed,\n",
    "        mini_batch_mask,\n",
    "        mini_batch_seq_lengths,\n",
    "        annealing_factor=1.0,\n",
    "    ):\n",
    "        # this is the number of time steps we need to process in the mini-batch\n",
    "        T_max = mini_batch.size(1)\n",
    "\n",
    "        # register all PyTorch (sub)modules with pyro\n",
    "        # this needs to happen in both the model and guide\n",
    "        pyro.module(\"dmm\", self)\n",
    "\n",
    "        # set z_prev = z_0 to setup the recursive conditioning in p(z_t | z_{t-1})\n",
    "        z_prev = self.z_0.expand(mini_batch.size(0), self.z_0.size(0))\n",
    "\n",
    "        # we enclose all the sample statements in the model in a plate.\n",
    "        # this marks that each datapoint is conditionally independent of the others\n",
    "        with pyro.plate(\"z_minibatch\", len(mini_batch)):\n",
    "            # sample the latents z and observed x's one time step at a time\n",
    "            # we wrap this loop in pyro.markov so that TraceEnum_ELBO can use multiple samples from the guide at each z\n",
    "            for t in pyro.markov(range(1, T_max + 1)):\n",
    "                # the next chunk of code samples z_t ~ p(z_t | z_{t-1})\n",
    "                # note that (both here and elsewhere) we use poutine.scale to take care\n",
    "                # of KL annealing. we use the mask() method to deal with raggedness\n",
    "                # in the observed data (i.e. different sequences in the mini-batch\n",
    "                # have different lengths)\n",
    "\n",
    "                # first compute the parameters of the diagonal gaussian distribution p(z_t | z_{t-1})\n",
    "                z_loc, z_scale = self.trans(z_prev)\n",
    "\n",
    "                # then sample z_t according to dist.Normal(z_loc, z_scale)\n",
    "                # note that we use the reshape method so that the univariate Normal distribution\n",
    "                # is treated as a multivariate Normal distribution with a diagonal covariance.\n",
    "                with poutine.scale(scale=annealing_factor):\n",
    "                    z_t = pyro.sample(\n",
    "                        \"z_%d\" % t,\n",
    "                        dist.Normal(z_loc, z_scale)\n",
    "                        .mask(mini_batch_mask[:, t - 1 : t])\n",
    "                        .to_event(1),\n",
    "                    )\n",
    "\n",
    "                # compute the probabilities that parameterize the bernoulli likelihood\n",
    "                emission_probs_t = self.emitter(z_t)\n",
    "                print(z_t.shape, emission_probs_t.shape)\n",
    "                # the next statement instructs pyro to observe x_t according to the\n",
    "                # bernoulli distribution p(x_t|z_t)\n",
    "                pyro.sample(\n",
    "                    \"obs\" % t,\n",
    "                    dist.Bernoulli(emission_probs_t)\n",
    "                    .mask(mini_batch_mask[:, t - 1 : t])\n",
    "                    .to_event(1),\n",
    "                    obs=mini_batch[:, t - 1, :],\n",
    "                )\n",
    "                # the latent sampled at this time step will be conditioned upon\n",
    "                # in the next time step so keep track of it\n",
    "                z_prev = z_t\n",
    "\n",
    "    # the guide q(z_{1:T} | x_{1:T}) (i.e. the variational distribution)\n",
    "    def guide(\n",
    "        self,\n",
    "        mini_batch,\n",
    "        mini_batch_reversed,\n",
    "        mini_batch_mask,\n",
    "        mini_batch_seq_lengths,\n",
    "        annealing_factor=1.0,\n",
    "    ):\n",
    "\n",
    "        # this is the number of time steps we need to process in the mini-batch\n",
    "        T_max = mini_batch.size(1)\n",
    "        # register all PyTorch (sub)modules with pyro\n",
    "        pyro.module(\"dmm\", self)\n",
    "\n",
    "        # if on gpu we need the fully broadcast view of the rnn initial state\n",
    "        # to be in contiguous gpu memory\n",
    "        h_0_contig = self.h_0.expand(\n",
    "            1, mini_batch.size(0), self.rnn.hidden_size\n",
    "        ).contiguous()\n",
    "        # push the observed x's through the rnn;\n",
    "        # rnn_output contains the hidden state at each time step\n",
    "        rnn_output, _ = self.rnn(mini_batch_reversed, h_0_contig)\n",
    "        # reverse the time-ordering in the hidden state and un-pack it\n",
    "        rnn_output = poly.pad_and_reverse(rnn_output, mini_batch_seq_lengths)\n",
    "        # set z_prev = z_q_0 to setup the recursive conditioning in q(z_t |...)\n",
    "        z_prev = self.z_q_0.expand(mini_batch.size(0), self.z_q_0.size(0))\n",
    "\n",
    "        # we enclose all the sample statements in the guide in a plate.\n",
    "        # this marks that each datapoint is conditionally independent of the others.\n",
    "        with pyro.plate(\"z_minibatch\", len(mini_batch)):\n",
    "            # sample the latents z one time step at a time\n",
    "            # we wrap this loop in pyro.markov so that TraceEnum_ELBO can use multiple samples from the guide at each z\n",
    "            for t in pyro.markov(range(1, T_max + 1)):\n",
    "                # the next two lines assemble the distribution q(z_t | z_{t-1}, x_{t:T})\n",
    "                z_loc, z_scale = self.combiner(z_prev, rnn_output[:, t - 1, :])\n",
    "                # if we are using normalizing flows, we apply the sequence of transformations\n",
    "                # parameterized by self.iafs to the base distribution defined in the previous line\n",
    "                # to yield a transformed distribution that we use for q(z_t|...)\n",
    "                z_dist = dist.Normal(z_loc, z_scale)\n",
    "                assert z_dist.event_shape == ()\n",
    "                assert z_dist.batch_shape[-2:] == (\n",
    "                    len(mini_batch),\n",
    "                    self.z_q_0.size(0),\n",
    "                )\n",
    "\n",
    "                # sample z_t from the distribution z_dist\n",
    "                with pyro.poutine.scale(scale=annealing_factor):\n",
    "                # when no normalizing flow used, \".to_event(1)\" indicates latent dimensions are independent\n",
    "                    z_t = pyro.sample(\n",
    "                        \"z_%d\" % t,\n",
    "                        z_dist.mask(mini_batch_mask[:, t - 1 : t]).to_event(1),\n",
    "                    )\n",
    "                # the latent sampled at this time step will be conditioned upon in the next time step\n",
    "                # so keep track of it\n",
    "                z_prev = z_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32]) torch.Size([20, 88])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not all arguments converted during string formatting",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 223\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39m# parse command-line arguments and execute the main method\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 223\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[6], line 203\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39m# process each mini-batch; this is where we take gradient steps\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mfor\u001b[39;00m which_mini_batch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_mini_batches):\n\u001b[0;32m--> 203\u001b[0m     epoch_nll \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m process_minibatch(epoch, which_mini_batch, shuffled_indices)\n\u001b[1;32m    205\u001b[0m \u001b[39m# report training diagnostics\u001b[39;00m\n\u001b[1;32m    206\u001b[0m times\u001b[39m.\u001b[39mappend(time\u001b[39m.\u001b[39mtime())\n",
      "Cell \u001b[0;32mIn[6], line 155\u001b[0m, in \u001b[0;36mmain.<locals>.process_minibatch\u001b[0;34m(epoch, which_mini_batch, shuffled_indices)\u001b[0m\n\u001b[1;32m    143\u001b[0m (\n\u001b[1;32m    144\u001b[0m     mini_batch,\n\u001b[1;32m    145\u001b[0m     mini_batch_reversed,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m     cuda\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    154\u001b[0m \u001b[39m# do an actual gradient step\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m loss \u001b[39m=\u001b[39m svi\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m    156\u001b[0m     mini_batch,\n\u001b[1;32m    157\u001b[0m     mini_batch_reversed,\n\u001b[1;32m    158\u001b[0m     mini_batch_mask,\n\u001b[1;32m    159\u001b[0m     mini_batch_seq_lengths,\n\u001b[1;32m    160\u001b[0m     annealing_factor,\n\u001b[1;32m    161\u001b[0m )\n\u001b[1;32m    162\u001b[0m \u001b[39m# keep track of the training loss\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyro/infer/svi.py:145\u001b[0m, in \u001b[0;36mSVI.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39m# get loss and compute gradients\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39mwith\u001b[39;00m poutine\u001b[39m.\u001b[39mtrace(param_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m param_capture:\n\u001b[0;32m--> 145\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_and_grads(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mguide, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    147\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\n\u001b[1;32m    148\u001b[0m     site[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39munconstrained() \u001b[39mfor\u001b[39;00m site \u001b[39min\u001b[39;00m param_capture\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mnodes\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[39m# actually perform gradient steps\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[39m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyro/infer/trace_elbo.py:140\u001b[0m, in \u001b[0;36mTrace_ELBO.loss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[39m# grab a trace from the generator\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[39mfor\u001b[39;00m model_trace, guide_trace \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_traces(model, guide, args, kwargs):\n\u001b[1;32m    141\u001b[0m     loss_particle, surrogate_loss_particle \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_differentiable_loss_particle(\n\u001b[1;32m    142\u001b[0m         model_trace, guide_trace\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_particle \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_particles\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyro/infer/elbo.py:236\u001b[0m, in \u001b[0;36mELBO._get_traces\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_particles):\n\u001b[0;32m--> 236\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_trace(model, guide, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyro/infer/trace_elbo.py:57\u001b[0m, in \u001b[0;36mTrace_ELBO._get_trace\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_trace\u001b[39m(\u001b[39mself\u001b[39m, model, guide, args, kwargs):\n\u001b[1;32m     53\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m    Returns a single trace from the guide, and the model that is run\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m    against it.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     model_trace, guide_trace \u001b[39m=\u001b[39m get_importance_trace(\n\u001b[1;32m     58\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mflat\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_plate_nesting, model, guide, args, kwargs\n\u001b[1;32m     59\u001b[0m     )\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     61\u001b[0m         check_if_enumerated(guide_trace)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyro/infer/enum.py:65\u001b[0m, in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, args, kwargs, detach)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[39mif\u001b[39;00m detach:\n\u001b[1;32m     64\u001b[0m         guide_trace\u001b[39m.\u001b[39mdetach_()\n\u001b[0;32m---> 65\u001b[0m     model_trace \u001b[39m=\u001b[39m poutine\u001b[39m.\u001b[39;49mtrace(\n\u001b[1;32m     66\u001b[0m         poutine\u001b[39m.\u001b[39;49mreplay(model, trace\u001b[39m=\u001b[39;49mguide_trace), graph_type\u001b[39m=\u001b[39;49mgraph_type\n\u001b[1;32m     67\u001b[0m     )\u001b[39m.\u001b[39;49mget_trace(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     70\u001b[0m     check_model_guide_match(model_trace, guide_trace, max_plate_nesting)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:198\u001b[0m, in \u001b[0;36mTraceHandler.get_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_trace\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    191\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39m    :returns: data structure\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m    :rtype: pyro.poutine.Trace\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39m    Calls this poutine and returns its trace instead of the function's return value.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsngr\u001b[39m.\u001b[39mget_trace()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyro/poutine/trace_messenger.py:174\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsngr\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39madd_node(\n\u001b[1;32m    171\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m_INPUT\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_INPUT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    172\u001b[0m )\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    176\u001b[0m     exc_type, exc_value, traceback \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyro/poutine/messenger.py:12\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_context_wrap\u001b[39m(context, fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     \u001b[39mwith\u001b[39;00m context:\n\u001b[0;32m---> 12\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[5], line 100\u001b[0m, in \u001b[0;36mDMM.model\u001b[0;34m(self, mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths, annealing_factor)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mprint\u001b[39m(z_t\u001b[39m.\u001b[39mshape, emission_probs_t\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     97\u001b[0m \u001b[39m# the next statement instructs pyro to observe x_t according to the\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m# bernoulli distribution p(x_t|z_t)\u001b[39;00m\n\u001b[1;32m     99\u001b[0m pyro\u001b[39m.\u001b[39msample(\n\u001b[0;32m--> 100\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mobs\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m t,\n\u001b[1;32m    101\u001b[0m     dist\u001b[39m.\u001b[39mBernoulli(emission_probs_t)\n\u001b[1;32m    102\u001b[0m     \u001b[39m.\u001b[39mmask(mini_batch_mask[:, t \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m : t])\n\u001b[1;32m    103\u001b[0m     \u001b[39m.\u001b[39mto_event(\u001b[39m1\u001b[39m),\n\u001b[1;32m    104\u001b[0m     obs\u001b[39m=\u001b[39mmini_batch[:, t \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, :],\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    106\u001b[0m \u001b[39m# the latent sampled at this time step will be conditioned upon\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39m# in the next time step so keep track of it\u001b[39;00m\n\u001b[1;32m    108\u001b[0m z_prev \u001b[39m=\u001b[39m z_t\n",
      "\u001b[0;31mTypeError\u001b[0m: not all arguments converted during string formatting"
     ]
    }
   ],
   "source": [
    "# setup, training, and evaluation\n",
    "def main():\n",
    "    # setup logging\n",
    "    # console = logging.StreamHandler()\n",
    "    # console.setLevel(logging.INFO)\n",
    "    # logging.getLogger(\"\").addHandler(console)\n",
    "\n",
    "    mini_batch_size = 20\n",
    "\n",
    "    data = poly.load_data(poly.JSB_CHORALES)\n",
    "    training_seq_lengths = data[\"train\"][\"sequence_lengths\"]\n",
    "    training_data_sequences = data[\"train\"][\"sequences\"]\n",
    "    test_seq_lengths = data[\"test\"][\"sequence_lengths\"]\n",
    "    test_data_sequences = data[\"test\"][\"sequences\"]\n",
    "    val_seq_lengths = data[\"valid\"][\"sequence_lengths\"]\n",
    "    val_data_sequences = data[\"valid\"][\"sequences\"]\n",
    "    N_train_data = len(training_seq_lengths)\n",
    "    N_train_time_slices = float(torch.sum(training_seq_lengths))\n",
    "    N_mini_batches = int(\n",
    "        N_train_data / mini_batch_size\n",
    "        + int(N_train_data % mini_batch_size > 0)\n",
    "    )\n",
    "\n",
    "    logging.info(\n",
    "        \"N_train_data: %d     avg. training seq. length: %.2f    N_mini_batches: %d\"\n",
    "        % (N_train_data, training_seq_lengths.float().mean(), N_mini_batches)\n",
    "    )\n",
    "\n",
    "    # how often we do validation/test evaluation during training\n",
    "    val_test_frequency = 50\n",
    "    # the number of samples we use to do the evaluation\n",
    "    n_eval_samples = 1\n",
    "\n",
    "    # package repeated copies of val/test data for faster evaluation\n",
    "    # (i.e. set us up for vectorization)\n",
    "    def rep(x):\n",
    "        rep_shape = torch.Size([x.size(0) * n_eval_samples]) + x.size()[1:]\n",
    "        repeat_dims = [1] * len(x.size())\n",
    "        repeat_dims[0] = n_eval_samples\n",
    "        return (\n",
    "            x.repeat(repeat_dims)\n",
    "            .reshape(n_eval_samples, -1)\n",
    "            .transpose(1, 0)\n",
    "            .reshape(rep_shape)\n",
    "        )\n",
    "\n",
    "    # get the validation/test data ready for the dmm: pack into sequences, etc.\n",
    "    val_seq_lengths = rep(val_seq_lengths)\n",
    "    test_seq_lengths = rep(test_seq_lengths)\n",
    "    (\n",
    "        val_batch,\n",
    "        val_batch_reversed,\n",
    "        val_batch_mask,\n",
    "        val_seq_lengths,\n",
    "    ) = poly.get_mini_batch(\n",
    "        torch.arange(n_eval_samples * val_data_sequences.shape[0]),\n",
    "        rep(val_data_sequences),\n",
    "        val_seq_lengths,\n",
    "        cuda=False,\n",
    "    )\n",
    "    (\n",
    "        test_batch,\n",
    "        test_batch_reversed,\n",
    "        test_batch_mask,\n",
    "        test_seq_lengths,\n",
    "    ) = poly.get_mini_batch(\n",
    "        torch.arange(n_eval_samples * test_data_sequences.shape[0]),\n",
    "        rep(test_data_sequences),\n",
    "        test_seq_lengths,\n",
    "        cuda=False,\n",
    "    )\n",
    "    rnn_dropout_rate = 0.1\n",
    "    # instantiate the dmm\n",
    "    dmm = DMM(\n",
    "        rnn_dropout_rate=rnn_dropout_rate,\n",
    "        use_cuda=False,\n",
    "    )\n",
    "    learning_rate = 0.0003\n",
    "    beta_1 = 0.96\n",
    "    beta_2 = 0.999\n",
    "    clip_norm = 10.0\n",
    "    lr_decay = 0.99996\n",
    "    weight_decay = 2.0\n",
    "    save_model = \"\"\n",
    "    save_opt = \"\"\n",
    "    load_opt = \"\"\n",
    "    load_model = \"\"\n",
    "    annealing_epochs = 10\n",
    "    minimum_annealing_factor = 0.2\n",
    "    # setup optimizer\n",
    "    adam_params = {\n",
    "        \"lr\": learning_rate,\n",
    "        \"betas\": (beta_1, beta_2),\n",
    "        \"clip_norm\": clip_norm,\n",
    "        \"lrd\": lr_decay,\n",
    "        \"weight_decay\": weight_decay,\n",
    "    }\n",
    "    adam = ClippedAdam(adam_params)\n",
    "\n",
    "    svi = SVI(dmm.model, dmm.guide, adam, loss=Trace_ELBO())\n",
    "\n",
    "    # now we're going to define some functions we need to form the main training loop\n",
    "\n",
    "    # saves the model and optimizer states to disk\n",
    "    def save_checkpoint():\n",
    "        logging.info(\"saving model to %s...\" % save_model)\n",
    "        torch.save(dmm.state_dict(), save_model)\n",
    "        logging.info(\"saving optimizer states to %s...\" % save_opt)\n",
    "        adam.save(save_opt)\n",
    "        logging.info(\"done saving model and optimizer checkpoints to disk.\")\n",
    "\n",
    "    # loads the model and optimizer states from disk\n",
    "    def load_checkpoint():\n",
    "        assert exists(load_opt) and exists(\n",
    "            load_model\n",
    "        ), \"--load-model and/or --load-opt misspecified\"\n",
    "        logging.info(\"loading model from %s...\" % load_model)\n",
    "        dmm.load_state_dict(torch.load(load_model))\n",
    "        logging.info(\"loading optimizer states from %s...\" % load_opt)\n",
    "        adam.load(load_opt)\n",
    "        logging.info(\"done loading model and optimizer states.\")\n",
    "\n",
    "    # prepare a mini-batch and take a gradient step to minimize -elbo\n",
    "    def process_minibatch(epoch, which_mini_batch, shuffled_indices):\n",
    "        if annealing_epochs > 0 and epoch < annealing_epochs:\n",
    "            # compute the KL annealing factor approriate for the current mini-batch in the current epoch\n",
    "            min_af = minimum_annealing_factor\n",
    "            annealing_factor = min_af + (1.0 - min_af) * (\n",
    "                float(which_mini_batch + epoch * N_mini_batches + 1)\n",
    "                / float(annealing_epochs * N_mini_batches)\n",
    "            )\n",
    "        else:\n",
    "            # by default the KL annealing factor is unity\n",
    "            annealing_factor = 1.0\n",
    "\n",
    "        # compute which sequences in the training set we should grab\n",
    "        mini_batch_start = which_mini_batch * mini_batch_size\n",
    "        mini_batch_end = np.min(\n",
    "            [(which_mini_batch + 1) * mini_batch_size, N_train_data]\n",
    "        )\n",
    "        mini_batch_indices = shuffled_indices[mini_batch_start:mini_batch_end]\n",
    "        # grab a fully prepped mini-batch using the helper function in the data loader\n",
    "        (\n",
    "            mini_batch,\n",
    "            mini_batch_reversed,\n",
    "            mini_batch_mask,\n",
    "            mini_batch_seq_lengths,\n",
    "        ) = poly.get_mini_batch(\n",
    "            mini_batch_indices,\n",
    "            training_data_sequences,\n",
    "            training_seq_lengths,\n",
    "            cuda=False,\n",
    "        )\n",
    "        # do an actual gradient step\n",
    "        loss = svi.step(\n",
    "            mini_batch,\n",
    "            mini_batch_reversed,\n",
    "            mini_batch_mask,\n",
    "            mini_batch_seq_lengths,\n",
    "            annealing_factor,\n",
    "        )\n",
    "        # keep track of the training loss\n",
    "        return loss\n",
    "\n",
    "    # helper function for doing evaluation\n",
    "    def do_evaluation():\n",
    "        # put the RNN into evaluation mode (i.e. turn off drop-out if applicable)\n",
    "        dmm.rnn.eval()\n",
    "\n",
    "        # compute the validation and test loss n_samples many times\n",
    "        val_nll = svi.evaluate_loss(\n",
    "            val_batch, val_batch_reversed, val_batch_mask, val_seq_lengths\n",
    "        ) / float(torch.sum(val_seq_lengths))\n",
    "        test_nll = svi.evaluate_loss(\n",
    "            test_batch, test_batch_reversed, test_batch_mask, test_seq_lengths\n",
    "        ) / float(torch.sum(test_seq_lengths))\n",
    "\n",
    "        # put the RNN back into training mode (i.e. turn on drop-out if applicable)\n",
    "        dmm.rnn.train()\n",
    "        return val_nll, test_nll\n",
    "\n",
    "    # if checkpoint files provided, load model and optimizer states from disk before we start training\n",
    "    if load_opt != \"\" and load_model != \"\":\n",
    "        load_checkpoint()\n",
    "\n",
    "    #################\n",
    "    # TRAINING LOOP #\n",
    "    #################\n",
    "    num_epochs = 50\n",
    "    times = [time.time()]\n",
    "    for epoch in range(num_epochs):\n",
    "        # if specified, save model and optimizer states to disk every checkpoint_freq epochs\n",
    "        if 0 > 0 and epoch > 0 and epoch % 0 == 0:\n",
    "            save_checkpoint()\n",
    "\n",
    "        # accumulator for our estimate of the negative log likelihood (or rather -elbo) for this epoch\n",
    "        epoch_nll = 0.0\n",
    "        # prepare mini-batch subsampling indices for this epoch\n",
    "        shuffled_indices = torch.randperm(N_train_data)\n",
    "\n",
    "        # process each mini-batch; this is where we take gradient steps\n",
    "        for which_mini_batch in range(N_mini_batches):\n",
    "            epoch_nll += process_minibatch(epoch, which_mini_batch, shuffled_indices)\n",
    "\n",
    "        # report training diagnostics\n",
    "        times.append(time.time())\n",
    "        epoch_time = times[-1] - times[-2]\n",
    "        logging.info(\n",
    "            \"[training epoch %04d]  %.4f \\t\\t\\t\\t(dt = %.3f sec)\"\n",
    "            % (epoch, epoch_nll / N_train_time_slices, epoch_time)\n",
    "        )\n",
    "\n",
    "        # do evaluation on test and validation data and report results\n",
    "        if val_test_frequency > 0 and epoch > 0 and epoch % val_test_frequency == 0:\n",
    "            val_nll, test_nll = do_evaluation()\n",
    "            logging.info(\n",
    "                \"[val/test epoch %04d]  %.4f  %.4f\" % (epoch, val_nll, test_nll)\n",
    "            )\n",
    "\n",
    "\n",
    "# parse command-line arguments and execute the main method\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8fadc5f936274309d5ca82691dfd61f89474af03c6ebe71b05790b9d22fb5245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
